{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf683b3",
   "metadata": {},
   "source": [
    "# 🧠 Entangled Learning 2.2: Alternating Teacher-Student on Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c063d95",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates **Entangled Learning 2.2**, where two models trained on different feature spaces take turns being the teacher and student. \n",
    "\n",
    "### Key Features:\n",
    "- Uses real-world **Breast Cancer Wisconsin dataset**\n",
    "- Teacher-Student roles alternate every epoch\n",
    "- Combines **Cross-Entropy Loss** with **KL divergence**\n",
    "- Visualizes loss and alignment dynamics\n",
    "- Compares performance via ROC AUC scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f96c97",
   "metadata": {},
   "source": [
    "## 📊 Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82932391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess data\n",
    "data = load_breast_cancer()\n",
    "X = StandardScaler().fit_transform(data.data)\n",
    "y = data.target\n",
    "X_pca = PCA(n_components=10).fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test, X_pca_train, X_pca_test = train_test_split(\n",
    "    X, y, X_pca, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e78dd",
   "metadata": {},
   "source": [
    "## 🧠 MLP Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95615748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "def make_mlp(input_dim, units=[64, 32]):\n",
    "    model = Sequential()\n",
    "    for i, u in enumerate(units):\n",
    "        if i == 0:\n",
    "            model.add(Dense(u, activation='relu', input_shape=(input_dim,)))\n",
    "        else:\n",
    "            model.add(Dense(u, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "mlp_A = make_mlp(X.shape[1])\n",
    "mlp_B = make_mlp(X_pca.shape[1], [32])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91af67d",
   "metadata": {},
   "source": [
    "## 🔁 Alternating Entangled Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "optimizer_A = tf.keras.optimizers.Adam()\n",
    "optimizer_B = tf.keras.optimizers.Adam()\n",
    "bce = BinaryCrossentropy()\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "history = {\"loss_A\": [], \"loss_B\": [], \"kl_A\": [], \"kl_B\": [], \"lambda\": []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    λ = 0.05 * (epoch / (epochs - 1))\n",
    "    history[\"lambda\"].append(λ)\n",
    "    idxs = np.random.permutation(len(X_train))\n",
    "    X_A, X_B, y_batch = X_train[idxs], X_pca_train[idxs], y_train[idxs]\n",
    "    loss_A_epoch, loss_B_epoch, kl_A_epoch, kl_B_epoch = [], [], [], []\n",
    "\n",
    "    for i in range(0, len(X_A), batch_size):\n",
    "        xb_A = X_A[i:i+batch_size]\n",
    "        xb_B = X_B[i:i+batch_size]\n",
    "        yb = y_batch[i:i+batch_size]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            out_A = mlp_A(xb_A, training=True)\n",
    "            out_B = mlp_B(xb_B, training=True)\n",
    "            out_A_np = out_A.numpy().flatten()\n",
    "            out_B_np = out_B.numpy().flatten()\n",
    "\n",
    "            ce_A = bce(yb, out_A)\n",
    "            ce_B = bce(yb, out_B)\n",
    "            kl_AB = tf.reduce_mean(out_B_np * tf.math.log(tf.clip_by_value(out_B_np / out_A_np, 1e-7, 1e7)))\n",
    "            kl_BA = tf.reduce_mean(out_A_np * tf.math.log(tf.clip_by_value(out_A_np / out_B_np, 1e-7, 1e7)))\n",
    "\n",
    "            if epoch % 2 == 0:\n",
    "                total_A = ce_A\n",
    "                total_B = ce_B + λ * kl_AB\n",
    "            else:\n",
    "                total_A = ce_A + λ * kl_BA\n",
    "                total_B = ce_B\n",
    "\n",
    "        grads_A = tape.gradient(total_A, mlp_A.trainable_weights)\n",
    "        grads_B = tape.gradient(total_B, mlp_B.trainable_weights)\n",
    "        optimizer_A.apply_gradients(zip(grads_A, mlp_A.trainable_weights))\n",
    "        optimizer_B.apply_gradients(zip(grads_B, mlp_B.trainable_weights))\n",
    "\n",
    "        loss_A_epoch.append(total_A.numpy())\n",
    "        loss_B_epoch.append(total_B.numpy())\n",
    "        kl_A_epoch.append(kl_BA.numpy())\n",
    "        kl_B_epoch.append(kl_AB.numpy())\n",
    "\n",
    "    history[\"loss_A\"].append(np.mean(loss_A_epoch))\n",
    "    history[\"loss_B\"].append(np.mean(loss_B_epoch))\n",
    "    history[\"kl_A\"].append(np.mean(kl_A_epoch))\n",
    "    history[\"kl_B\"].append(np.mean(kl_B_epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0b501",
   "metadata": {},
   "source": [
    "## 📈 Training Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history[\"loss_A\"], label=\"MLP_A (original input) Loss\")\n",
    "plt.plot(history[\"loss_B\"], label=\"MLP_B (PCA input) Loss\")\n",
    "plt.plot(history[\"kl_A\"], '--', label=\"KL(A aligned to B)\")\n",
    "plt.plot(history[\"kl_B\"], '--', label=\"KL(B aligned to A)\")\n",
    "plt.plot(history[\"lambda\"], label=\"λ (Entanglement)\", color='gray', linestyle=':')\n",
    "plt.title(\"Entangled Learning 2.2 — Alternating Teacher-Student on Real Data\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss / KL\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70999bf",
   "metadata": {},
   "source": [
    "## ✅ Evaluation & ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67e65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_pred_A = mlp_A.predict(X_test).flatten()\n",
    "y_pred_B = mlp_B.predict(X_pca_test).flatten()\n",
    "\n",
    "fpr_A, tpr_A, _ = roc_curve(y_test, y_pred_A)\n",
    "fpr_B, tpr_B, _ = roc_curve(y_test, y_pred_B)\n",
    "auc_A = auc(fpr_A, tpr_A)\n",
    "auc_B = auc(fpr_B, tpr_B)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_A, tpr_A, label=f\"MLP_A ROC (AUC={auc_A:.2f})\")\n",
    "plt.plot(fpr_B, tpr_B, label=f\"MLP_B ROC (AUC={auc_B:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.title(\"ROC Curves for MLP_A and MLP_B\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"MLP_A (original input) Test Accuracy:\", np.mean((y_pred_A > 0.5) == y_test))\n",
    "print(\"MLP_B (PCA input) Test Accuracy:\", np.mean((y_pred_B > 0.5) == y_test))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
